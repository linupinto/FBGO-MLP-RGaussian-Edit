# A brief Over view of the programme BN_ReLu_cifar.py

## Key features
- Implements Batch Normalization with full batch processing
- Input normalization at each node using moving average and variance
- Full Gradient Optimization method is consisderd
- Two hidden layer and a single output layer
- ReLu activation in each node of the hidden layer
- Linear activation in the output layer
- Uses standard normalized CIFAR data

### Manin highlight
- Thsis can be extented to any number of hidden layers and nodes.
