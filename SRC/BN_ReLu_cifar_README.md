# A brief Over view of the programme BN_ReLu_cifar.py

## Key features
-  Implements Batch Normalization with full batch processing
-  Input normalization at each node using moving average and variance
-  Full Gradient Optimization method is considered
-  For experimental analysis two hidden layer and a single output layer is structured in the code.
    - The code  will work on any number of hidden layes and hidden nodes.
-  ReLu activation in each node of the hidden layer
-  Linear activation in the output layer
-  Uses standard normalized CIFAR data

### Main highlight
- The code is implemented for any number of hidden layers and nodes.
